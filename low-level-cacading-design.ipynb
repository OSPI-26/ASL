{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":46105,"databundleVersionId":5087314,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\nfrom scipy.interpolate import interp1d\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nfrom sklearn.metrics import accuracy_score, f1_score\nimport matplotlib.pyplot as plt\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nBASE_PATH = \"/kaggle/input/asl-signs\"\nPREPROCESSED_PATH = \"./preprocessed_data\"\n\n# Load metadata\ntrain_csv = pd.read_csv(f\"{BASE_PATH}/train.csv\")\nwith open(f\"{BASE_PATH}/sign_to_prediction_index_map.json\", \"r\") as f:\n    label_map = json.load(f)\n\nNUM_CLASSES = len(label_map)\nprint(f\"Classes: {NUM_CLASSES}\")\n\nN_JOBS = 4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef has_valid_hands_fast(sample_path, min_frames=5):\n    \"\"\"Filter samples with insufficient hand data\"\"\"\n    full_path = os.path.join(BASE_PATH, sample_path)\n    df = pd.read_parquet(full_path, columns=[\"frame\", \"type\"], engine=\"pyarrow\")\n    \n    hand_df = df[df[\"type\"].isin([\"left_hand\", \"right_hand\"])]\n    if hand_df.empty or hand_df[\"frame\"].nunique() < min_frames:\n        return False\n    return True\n\ndef check_index(i):\n    return i if has_valid_hands_fast(train_csv.iloc[i][\"path\"]) else None\n\n# Try loading pre-computed indices, otherwise compute\ntry:\n    valid_indices = np.load(\"/kaggle/input/valid-indices-npy/valid_indices.npy\").tolist()\n    print(f\"Loaded {len(valid_indices)} valid indices from cache\")\nexcept:\n    print(\"Computing valid indices (this may take a while)...\")\n    results = Parallel(n_jobs=N_JOBS, prefer=\"threads\")(\n        delayed(check_index)(i) for i in tqdm(range(len(train_csv)))\n    )\n    valid_indices = [i for i in results if i is not None]\n    os.makedirs(\"/kaggle/working\", exist_ok=True)\n    np.save(\"/kaggle/working/valid_indices.npy\", np.array(valid_indices))\n    print(f\"Valid samples: {len(valid_indices)} / {len(train_csv)}\")\n\n# Filter CSV to valid samples\ntrain_csv = train_csv.iloc[valid_indices].reset_index(drop=True)\nprint(f\"Filtered train_csv length: {len(train_csv)}\")\n\n\nclass LandmarkPreprocessor:\n    @staticmethod\n    def filter_empty_frames(kp, threshold=0.5):\n        missing_per_frame = np.sum(kp == 0, axis=(1, 2)) / (kp.shape[1] * 3)\n        valid_mask = missing_per_frame < threshold\n        if np.sum(valid_mask) == 0:\n            return kp[:1]\n        return kp[valid_mask]\n    \n    @staticmethod\n    def interpolate_missing(kp):\n        T, L, C = kp.shape\n        kp_interp = kp.copy()\n        \n        for l in range(L):\n            for c in range(C):\n                track = kp[:, l, c]\n                valid_mask = track != 0\n                n_valid = np.sum(valid_mask)\n                \n                if n_valid == 0:\n                    continue\n                elif n_valid == 1:\n                    kp_interp[:, l, c] = track[valid_mask][0]\n                else:\n                    valid_indices = np.where(valid_mask)[0]\n                    valid_values = track[valid_mask]\n                    f = interp1d(valid_indices, valid_values, kind='linear',\n                               fill_value='extrapolate', bounds_error=False)\n                    kp_interp[:, l, c] = f(np.arange(T))\n        \n        return kp_interp\n    \n    @staticmethod\n    def anchor_normalize(kp):\n        T, L, C = kp.shape\n        \n        # For hand landmarks, use centroid of all valid points\n        valid_mask = kp != 0\n        \n        # Calculate centroid per frame\n        anchor_pos = np.zeros((T, C))\n        scale = np.zeros(T)\n        \n        for t in range(T):\n            valid_points = kp[t][np.any(valid_mask[t], axis=1)]\n            if len(valid_points) > 0:\n                anchor_pos[t] = valid_points.mean(axis=0)\n                # Scale based on spread of points\n                distances = np.linalg.norm(valid_points - anchor_pos[t], axis=1)\n                scale[t] = distances.mean() if len(distances) > 0 else 1.0\n            else:\n                anchor_pos[t] = 0\n                scale[t] = 1.0\n        \n        # Apply normalization\n        anchor_pos_expanded = anchor_pos[:, np.newaxis, :]\n        kp_relative = kp - anchor_pos_expanded\n        \n        # Use mean scale to avoid division by zero\n        mean_scale = scale[scale > 0].mean() if np.any(scale > 0) else 1.0\n        if mean_scale > 1e-6:\n            kp_normalized = kp_relative / mean_scale\n        else:\n            kp_normalized = kp_relative\n        \n        return kp_normalized, anchor_pos, mean_scale\n\nclass LandmarkAugmentor:\n    @staticmethod\n    def horizontal_flip(kp, left_hand_indices, right_hand_indices, p=0.5):\n        \"\"\"FIXED: Proper handling of hand indices\"\"\"\n        if np.random.rand() < p:\n            kp = kp.copy()\n            kp[:, :, 0] *= -1  # Flip x-coordinate\n            \n            # Swap hands if both are present\n            if len(left_hand_indices) > 0 and len(right_hand_indices) > 0:\n                left_hand = kp[:, left_hand_indices, :].copy()\n                right_hand = kp[:, right_hand_indices, :].copy()\n                kp[:, left_hand_indices, :] = right_hand\n                kp[:, right_hand_indices, :] = left_hand\n        return kp\n    \n    @staticmethod\n    def rotation_3d(kp, max_angle=60, p=0.5):\n        if np.random.rand() < p:\n            angle = np.random.uniform(-max_angle, max_angle)\n            angle_rad = np.deg2rad(angle)\n            \n            cos_a, sin_a = np.cos(angle_rad), np.sin(angle_rad)\n            R = np.array([[cos_a, -sin_a, 0], [sin_a, cos_a, 0], [0, 0, 1]])\n            \n            T, L, _ = kp.shape\n            kp_flat = kp.reshape(-1, 3)\n            kp_rotated = (R @ kp_flat.T).T\n            kp = kp_rotated.reshape(T, L, 3)\n        return kp\n    \n    @staticmethod\n    def resize_3d(kp, scale_range=(0.8, 1.2), p=0.5):\n        if np.random.rand() < p:\n            scale = np.random.uniform(*scale_range)\n            kp = kp * scale\n        return kp\n    \n    @staticmethod\n    def finger_dropout(kp, hand_indices, p_per_finger=0.1, p=0.3):\n        if np.random.rand() < p:\n            for idx in hand_indices:\n                if np.random.rand() < p_per_finger:\n                    kp[:, idx, :] = 0\n        return kp\n\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n    \n    def forward(self, logits, targets):\n        probs = F.softmax(logits, dim=1)\n        targets_one_hot = F.one_hot(targets, num_classes=logits.shape[1])\n        p_t = (probs * targets_one_hot).sum(dim=1)\n        \n        focal_weight = (1 - p_t) ** self.gamma\n        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n        focal_loss = focal_weight * ce_loss\n        \n        if self.alpha is not None:\n            if self.alpha.device != logits.device:\n                self.alpha = self.alpha.to(logits.device)\n            alpha_t = self.alpha[targets]\n            focal_loss = alpha_t * focal_loss\n        \n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        return focal_loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ASLDataset(Dataset):\n    def __init__(self, dataframe, label_map, max_frames=64, split='train',\n                 use_anchor_norm=True, use_interpolation=True, use_augmentation=True):\n        self.df = dataframe.reset_index(drop=True)\n        self.label_map = label_map\n        self.max_frames = max_frames\n        self.split = split\n        self.use_anchor_norm = use_anchor_norm\n        self.use_interpolation = use_interpolation\n        self.use_augmentation = use_augmentation and (split == 'train')\n        \n        # Initialize helpers\n        self.preprocessor = LandmarkPreprocessor()\n        self.augmentor = LandmarkAugmentor()\n        \n        # CRITICAL: Define hand indices (42 landmarks total, 21 per hand)\n        self.left_hand_indices = list(range(0, 21))\n        self.right_hand_indices = list(range(21, 42))\n        \n        # Class balancing\n        self.class_counts = self._analyze_class_distribution()\n        self.class_weights = self._compute_class_weights()\n        \n\n        print(f\"ASL Dataset - {split.upper()}\")\n        \n        print(f\"Samples: {len(self.df)}\")\n        print(f\"Classes: {len(label_map)}\")\n        print(f\"Max frames: {max_frames}\")\n        print(f\"Anchor normalization: {use_anchor_norm}\")\n        print(f\"Interpolation: {use_interpolation}\")\n        print(f\"Augmentation: {self.use_augmentation}\")\n\n    \n    def _analyze_class_distribution(self):\n        labels = [self.label_map[row['sign']] for _, row in self.df.iterrows()]\n        return Counter(labels)\n    \n    def _compute_class_weights(self):\n        num_classes = len(self.label_map)\n        weights = np.zeros(num_classes)\n        total_samples = sum(self.class_counts.values())\n        \n        for class_idx in range(num_classes):\n            count = self.class_counts.get(class_idx, 1)\n            weights[class_idx] = total_samples / (num_classes * count)\n        \n        weights = weights / weights.sum() * num_classes\n        return torch.FloatTensor(weights)\n    \n    def get_sample_weights(self):\n        \"\"\"For WeightedRandomSampler\"\"\"\n        sample_weights = []\n        for _, row in self.df.iterrows():\n            label = self.label_map[row['sign']]\n            sample_weights.append(self.class_weights[label].item())\n        return torch.FloatTensor(sample_weights)\n    \n    def load_hand_keypoints(self, sample_path):\n        \"\"\"Load hand landmarks (42 landmarks: 21 left + 21 right)\"\"\"\n        full_path = os.path.join(BASE_PATH, sample_path)\n        df = pd.read_parquet(full_path, engine=\"pyarrow\")\n        \n        hand_df = df[df[\"type\"].isin([\"left_hand\", \"right_hand\"])]\n        hand_df = hand_df.sort_values([\"frame\", \"type\", \"landmark_index\"])\n        \n        frames = hand_df[\"frame\"].nunique()\n        coords = hand_df[[\"x\", \"y\", \"z\"]].values\n        coords = coords.reshape(frames, 42, 3)\n        \n        return np.nan_to_num(coords, nan=0.0)\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        # Load keypoints\n        kp = self.load_hand_keypoints(row[\"path\"])  # (T, 42, 3)\n        original_length = kp.shape[0]\n        \n        # Preprocessing\n        kp = self.preprocessor.filter_empty_frames(kp, threshold=0.5)\n        \n        if self.use_interpolation:\n            kp = self.preprocessor.interpolate_missing(kp)\n        \n        if self.use_anchor_norm:\n            kp, _, _ = self.preprocessor.anchor_normalize(kp)\n        \n        # Temporal resampling\n        if kp.shape[0] < self.max_frames:\n            pad_len = self.max_frames - kp.shape[0]\n            kp = np.concatenate([kp, np.zeros((pad_len, 42, 3))], axis=0)\n        elif kp.shape[0] > self.max_frames:\n            indices = np.linspace(0, kp.shape[0]-1, self.max_frames).astype(int)\n            kp = kp[indices]\n        \n        kp = np.nan_to_num(kp, nan=0.0)\n        \n        # Augmentation (FIXED with correct indices)\n        if self.use_augmentation:\n            kp = self.augmentor.horizontal_flip(\n                kp, self.left_hand_indices, self.right_hand_indices, p=0.5)\n            kp = self.augmentor.rotation_3d(kp, max_angle=60, p=0.5)\n            kp = self.augmentor.resize_3d(kp, scale_range=(0.8, 1.2), p=0.5)\n            \n            all_hand_indices = self.left_hand_indices + self.right_hand_indices\n            kp = self.augmentor.finger_dropout(kp, all_hand_indices, p=0.3)\n        \n        label = self.label_map[row[\"sign\"]]\n        \n        return torch.FloatTensor(kp), label\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_dataloaders(train_df, val_df, label_map, batch_size=32, \n                       max_frames=64, num_workers=4):\n    \n    train_ds = ASLDataset(\n        train_df, label_map, max_frames=max_frames, split='train',\n        use_anchor_norm=True, use_interpolation=True, use_augmentation=True\n    )\n    \n    val_ds = ASLDataset(\n        val_df, label_map, max_frames=max_frames, split='val',\n        use_anchor_norm=True, use_interpolation=True, use_augmentation=False\n    )\n    \n    # Weighted sampler for balanced training\n    sample_weights = train_ds.get_sample_weights()\n    sampler = WeightedRandomSampler(\n        weights=sample_weights,\n        num_samples=len(sample_weights),\n        replacement=True\n    )\n    \n    train_loader = DataLoader(\n        train_ds, batch_size=batch_size, sampler=sampler,\n        num_workers=num_workers, pin_memory=True,\n        prefetch_factor=2, persistent_workers=True, drop_last=True\n    )\n    \n    val_loader = DataLoader(\n        val_ds, batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=True,\n        prefetch_factor=2, persistent_workers=True\n    )\n    \n    \n    focal_loss = FocalLoss(alpha=train_ds.class_weights, gamma=2.0)\n    \n    return train_loader, val_loader, focal_loss, train_ds.class_weights\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    set_seed(42)\n    \n    split_idx = int(len(train_csv) * 0.8)\n    train_df = train_csv[:split_idx]\n    val_df = train_csv[split_idx:]\n    \n    print(f\"Train samples: {len(train_df)}, Val samples: {len(val_df)}\")\n    \n    train_loader, val_loader, focal_loss, class_weights = create_dataloaders(\n        train_df, val_df, label_map, \n        batch_size=32, max_frames=64, num_workers=4\n    )\n    \n    for batch_idx, (data, labels) in enumerate(train_loader):\n        print(f\"Batch shape: {data.shape}\")  # Should be [32, 64, 42, 3]\n        print(f\"Labels shape: {labels.shape}\")\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nfrom sklearn.metrics import accuracy_score, f1_score\nimport matplotlib.pyplot as plt\nimport math\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def stratified_sample_dataset(train_csv, sample_ratio=0.75, random_state=42):\n    \"\"\"\n    Sample dataset while preserving class distribution\n    \n    Args:\n        train_csv: DataFrame with 'path' and 'sign' columns\n        sample_ratio: Fraction of data to keep (0.5 = 50%)\n        random_state: Random seed for reproducibility\n    \n    Returns:\n        sampled_df: Stratified sample of the dataset\n    \"\"\"\n    print(f\"Stratified Sampling: Keeping {sample_ratio*100:.0f}% of data\")\n    \n    # Group by class and sample proportionally\n    sampled_dfs = []\n    \n    for sign_class in train_csv['sign'].unique():\n        class_df = train_csv[train_csv['sign'] == sign_class]\n        n_samples = max(1, int(len(class_df) * sample_ratio))  # At least 1 sample per class\n        \n        sampled_class = class_df.sample(n=n_samples, random_state=random_state)\n        sampled_dfs.append(sampled_class)\n    \n    sampled_df = pd.concat(sampled_dfs, ignore_index=True)\n    sampled_df = sampled_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n    \n    print(f\"Original dataset: {len(train_csv)} samples\")\n    print(f\"Sampled dataset: {len(sampled_df)} samples\")\n    print(f\"Reduction: {(1 - len(sampled_df)/len(train_csv))*100:.1f}%\")\n    \n    orig_dist = train_csv['sign'].value_counts(normalize=True).sort_index()\n    samp_dist = sampled_df['sign'].value_counts(normalize=True).sort_index()\n    \n    max_deviation = (orig_dist - samp_dist).abs().max()\n    print(f\"Max class distribution deviation: {max_deviation*100:.2f}%\")\n    print(f\"{'='*60}\\n\")\n    \n    return sampled_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DifficultyAnalyzer:\n    \"\"\"\n    Multi-dimensional difficulty scoring for intelligent cascading\n    Combines: prediction uncertainty, spatial complexity, temporal complexity, motion patterns\n    \"\"\"\n    \n    @staticmethod\n    def compute_prediction_uncertainty(logits):\n        \"\"\"Entropy-based uncertainty from model predictions\"\"\"\n        probs = F.softmax(logits, dim=1)\n        entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1)\n        max_entropy = math.log(logits.shape[1])\n        return entropy / max_entropy  # [0, 1]\n    \n    @staticmethod\n    def compute_spatial_complexity(x):\n        \"\"\"\n        Measure spatial spread and hand coordination complexity\n        Args: x shape (batch, max_len, 42, 3)\n        Returns: complexity score [0, 1]\n        \"\"\"\n        batch_size = x.shape[0]\n        complexity_scores = []\n        \n        for i in range(batch_size):\n            sample = x[i]  # (max_len, 42, 3)\n            \n            # Get valid frames (non-zero)\n            valid_mask = sample.abs().sum(dim=(1, 2)) > 0\n            if valid_mask.sum() == 0:\n                complexity_scores.append(0.0)\n                continue\n            \n            valid_frames = sample[valid_mask]  # (T, 42, 3)\n            \n            # 1. Hand spread (variance of landmark positions)\n            spread = valid_frames.std(dim=1).mean().item()  # Higher = more spread out\n            \n            # 2. Two-hand coordination (difference between left and right hand)\n            left_hand = valid_frames[:, :21, :]  # First 21 landmarks\n            right_hand = valid_frames[:, 21:, :]  # Last 21 landmarks\n            \n            left_center = left_hand.mean(dim=1)\n            right_center = right_hand.mean(dim=1)\n            hand_distance = torch.norm(left_center - right_center, dim=1).mean().item()\n            \n            # 3. Finger articulation (variance within each hand)\n            left_articulation = left_hand.std(dim=1).mean().item()\n            right_articulation = right_hand.std(dim=1).mean().item()\n            articulation = (left_articulation + right_articulation) / 2\n            \n            # Normalize and combine (higher values = more complex)\n            spatial_complexity = (spread * 0.3 + hand_distance * 0.4 + articulation * 0.3)\n            # Clip to [0, 1] range (empirically tuned)\n            spatial_complexity = min(spatial_complexity / 2.0, 1.0)\n            \n            complexity_scores.append(spatial_complexity)\n        \n        return torch.tensor(complexity_scores, device=x.device)\n    \n    @staticmethod\n    def compute_temporal_complexity(x):\n        \"\"\"\n        Measure temporal variation and motion smoothness\n        Args: x shape (batch, max_len, 42, 3)\n        Returns: complexity score [0, 1]\n        \"\"\"\n        batch_size = x.shape[0]\n        complexity_scores = []\n        \n        for i in range(batch_size):\n            sample = x[i]  # (max_len, 42, 3)\n            \n            # Get valid frames\n            valid_mask = sample.abs().sum(dim=(1, 2)) > 0\n            if valid_mask.sum() <= 1:\n                complexity_scores.append(0.0)\n                continue\n            \n            valid_frames = sample[valid_mask]  # (T, 42, 3)\n            T = valid_frames.shape[0]\n            \n            # 1. Frame-to-frame velocity (first derivative)\n            velocity = torch.diff(valid_frames, dim=0)  # (T-1, 42, 3)\n            avg_velocity = velocity.abs().mean().item()\n            \n            # 2. Acceleration (second derivative) - jerkiness\n            if T > 2:\n                acceleration = torch.diff(velocity, dim=0)  # (T-2, 42, 3)\n                avg_acceleration = acceleration.abs().mean().item()\n            else:\n                avg_acceleration = 0.0\n            \n            # 3. Temporal variance (how much the sign changes over time)\n            temporal_variance = valid_frames.std(dim=0).mean().item()\n            \n            # 4. Direction changes (non-smooth motion)\n            if T > 2:\n                velocity_norm = F.normalize(velocity.reshape(T-1, -1), dim=1)\n                direction_changes = (1 - (velocity_norm[:-1] * velocity_norm[1:]).sum(dim=1)).mean().item()\n            else:\n                direction_changes = 0.0\n            \n            # Combine metrics (higher = more complex temporal pattern)\n            temporal_complexity = (\n                avg_velocity * 0.3 + \n                avg_acceleration * 0.3 + \n                temporal_variance * 0.2 + \n                direction_changes * 0.2\n            )\n            # Normalize to [0, 1]\n            temporal_complexity = min(temporal_complexity / 1.5, 1.0)\n            \n            complexity_scores.append(temporal_complexity)\n        \n        return torch.tensor(complexity_scores, device=x.device)\n    \n    @staticmethod\n    def compute_motion_pattern_complexity(x):\n        \"\"\"\n        Analyze motion patterns (circular, linear, static, etc.)\n        Args: x shape (batch, max_len, 42, 3)\n        Returns: complexity score [0, 1]\n        \"\"\"\n        batch_size = x.shape[0]\n        complexity_scores = []\n        \n        for i in range(batch_size):\n            sample = x[i]\n            \n            valid_mask = sample.abs().sum(dim=(1, 2)) > 0\n            if valid_mask.sum() <= 2:\n                complexity_scores.append(0.0)\n                continue\n            \n            valid_frames = sample[valid_mask]\n            T = valid_frames.shape[0]\n            \n            # Use hand centroids for motion pattern\n            hand_centroid = valid_frames.mean(dim=1)  # (T, 3)\n            \n            # 1. Path length vs straight-line distance (tortuosity)\n            path_length = torch.norm(torch.diff(hand_centroid, dim=0), dim=1).sum().item()\n            straight_distance = torch.norm(hand_centroid[-1] - hand_centroid[0]).item()\n            \n            if straight_distance > 1e-6:\n                tortuosity = path_length / straight_distance\n            else:\n                tortuosity = 1.0\n            \n            # 2. 3D motion (z-axis usage)\n            z_variance = hand_centroid[:, 2].std().item()\n            \n            # 3. Circular vs linear motion (variance in different axes)\n            xy_variance = hand_centroid[:, :2].std(dim=0).mean().item()\n            \n            # Combine (complex motions have high tortuosity and 3D usage)\n            motion_complexity = (\n                min(tortuosity / 3.0, 1.0) * 0.5 +\n                min(z_variance / 0.3, 1.0) * 0.3 +\n                min(xy_variance / 0.5, 1.0) * 0.2\n            )\n            \n            complexity_scores.append(motion_complexity)\n        \n        return torch.tensor(complexity_scores, device=x.device)\n    \n    @classmethod\n    def compute_difficulty_score(cls, x, logits, weights=None):\n        \"\"\"\n        Compute comprehensive difficulty score combining all factors\n        \n        Args:\n            x: input data (batch, max_len, 42, 3)\n            logits: model predictions (batch, num_classes)\n            weights: dict of weights for each component\n        \n        Returns:\n            difficulty_score: [0, 1] where higher = more difficult\n            components: dict with individual scores for analysis\n        \"\"\"\n        if weights is None:\n            weights = {\n                'prediction_uncertainty': 0.35,\n                'spatial_complexity': 0.25,\n                'temporal_complexity': 0.25,\n                'motion_complexity': 0.15\n            }\n        \n        # Compute all components\n        pred_uncertainty = cls.compute_prediction_uncertainty(logits)\n        spatial_comp = cls.compute_spatial_complexity(x)\n        temporal_comp = cls.compute_temporal_complexity(x)\n        motion_comp = cls.compute_motion_pattern_complexity(x)\n        \n        # Weighted combination\n        difficulty = (\n            weights['prediction_uncertainty'] * pred_uncertainty +\n            weights['spatial_complexity'] * spatial_comp +\n            weights['temporal_complexity'] * temporal_comp +\n            weights['motion_complexity'] * motion_comp\n        )\n        \n        components = {\n            'prediction_uncertainty': pred_uncertainty,\n            'spatial_complexity': spatial_comp,\n            'temporal_complexity': temporal_comp,\n            'motion_complexity': motion_comp,\n            'overall_difficulty': difficulty\n        }\n        \n        return difficulty, components\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass Conv1DBlock(nn.Module):\n    \"\"\"1D Convolutional block with depthwise separable convolution\"\"\"\n    def __init__(self, dim, kernel_size=17, drop_rate=0.2):\n        super().__init__()\n        padding = kernel_size // 2\n        \n        self.dwconv = nn.Conv1d(dim, dim, kernel_size, padding=padding, groups=dim)\n        self.pwconv = nn.Conv1d(dim, dim, 1)\n        self.bn = nn.BatchNorm1d(dim, momentum=0.05)\n        self.dropout = nn.Dropout(drop_rate)\n        \n    def forward(self, x):\n        residual = x\n        x = self.dwconv(x)\n        x = self.pwconv(x)\n        x = self.bn(x)\n        x = F.gelu(x)\n        x = self.dropout(x)\n        return x + residual\n\n\nclass LightweightTransformerBlock(nn.Module):\n    \"\"\"Lightweight transformer block for temporal modeling\"\"\"\n    def __init__(self, dim, num_heads=4, expand=2, dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)\n        self.norm2 = nn.LayerNorm(dim)\n        \n        self.ffn = nn.Sequential(\n            nn.Linear(dim, dim * expand),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim * expand, dim),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, x):\n        x = x.transpose(1, 2)\n        normed = self.norm1(x)\n        attn_out, _ = self.attn(normed, normed, normed)\n        x = x + attn_out\n        x = x + self.ffn(self.norm2(x))\n        return x.transpose(1, 2)\n\n\nclass LateDropout(nn.Module):\n    \"\"\"Dropout that only activates after a certain training step\"\"\"\n    def __init__(self, p=0.8, start_step=0):\n        super().__init__()\n        self.p = p\n        self.start_step = start_step\n        self.current_step = 0\n    \n    def forward(self, x):\n        if self.training and self.current_step >= self.start_step:\n            return F.dropout(x, p=self.p, training=True)\n        return x\n    \n    def step(self):\n        self.current_step += 1\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ASLFilterModel(nn.Module):\n    \"\"\"\n    Lightweight first-pass filter with multi-dimensional difficulty assessment\n    \"\"\"\n    def __init__(self, num_classes, max_len=64, channels=126, dim=96, dropout_step=0):\n        super().__init__()\n        self.channels = channels\n        self.pad_value = 0.0\n        \n        # Stem\n        self.stem_conv = nn.Linear(channels, dim, bias=False)\n        self.stem_bn = nn.BatchNorm1d(dim, momentum=0.05)\n        \n        # First block: 3 Conv1D + 1 Transformer\n        self.conv_block1 = nn.Sequential(\n            Conv1DBlock(dim, kernel_size=17, drop_rate=0.2),\n            Conv1DBlock(dim, kernel_size=17, drop_rate=0.2),\n            Conv1DBlock(dim, kernel_size=17, drop_rate=0.2),\n        )\n        self.transformer1 = LightweightTransformerBlock(dim, num_heads=4, expand=2)\n        \n        # Second block: 3 Conv1D + 1 Transformer\n        self.conv_block2 = nn.Sequential(\n            Conv1DBlock(dim, kernel_size=17, drop_rate=0.2),\n            Conv1DBlock(dim, kernel_size=17, drop_rate=0.2),\n            Conv1DBlock(dim, kernel_size=17, drop_rate=0.2),\n        )\n        self.transformer2 = LightweightTransformerBlock(dim, num_heads=4, expand=2)\n        \n        # Top conv and classification\n        self.top_conv = nn.Linear(dim, dim * 2)\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        self.late_dropout = LateDropout(0.8, start_step=dropout_step)\n        self.classifier = nn.Linear(dim * 2, num_classes)\n        \n        # Difficulty analyzer\n        self.difficulty_analyzer = DifficultyAnalyzer()\n        \n        # Better initialization for deep network\n        self._initialize_weights()\n        \n    def _initialize_weights(self):\n        \"\"\"Proper weight initialization for better convergence\"\"\"\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.trunc_normal_(m.weight, std=0.02)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Conv1d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, (nn.BatchNorm1d, nn.LayerNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        \n    def forward(self, x, return_difficulty=False, difficulty_weights=None):\n        \"\"\"\n        Args:\n            x: (batch, max_len, 42, 3) - raw landmark data\n            return_difficulty: If True, returns (logits, difficulty_score, components)\n            difficulty_weights: Custom weights for difficulty components\n        \"\"\"\n        batch_size = x.shape[0]\n        x_original = x.clone()  # Keep for difficulty analysis\n        \n        # Reshape: (batch, max_len, 42, 3) -> (batch, max_len, 126)\n        x = x.reshape(batch_size, x.shape[1], -1)\n        \n        # Stem\n        x = self.stem_conv(x)\n        x = x.transpose(1, 2)\n        x = self.stem_bn(x)\n        \n        # First stage\n        x = self.conv_block1(x)\n        x = self.transformer1(x)\n        \n        # Second stage\n        x = self.conv_block2(x)\n        x = self.transformer2(x)\n        \n        # Top conv and pooling\n        x = x.transpose(1, 2)\n        x = self.top_conv(x)\n        x = F.gelu(x)\n        x = x.transpose(1, 2)\n        x = self.global_pool(x).squeeze(-1)\n        \n        # Classification\n        x = self.late_dropout(x)\n        logits = self.classifier(x)\n        \n        if return_difficulty:\n            difficulty, components = self.difficulty_analyzer.compute_difficulty_score(\n                x_original, logits, difficulty_weights\n            )\n            return logits, difficulty, components\n        \n        return logits\n    \n    def get_model_size_mb(self):\n        \"\"\"Calculate model size in MB\"\"\"\n        param_size = sum(p.numel() * p.element_size() for p in self.parameters())\n        buffer_size = sum(b.numel() * b.element_size() for b in self.buffers())\n        size_mb = (param_size + buffer_size) / (1024 ** 2)\n        return size_mb\n\ndef train_filter_model(model, train_loader, val_loader, criterion, \n                       num_epochs=40, lr=1e-3, device='cuda',\n                       difficulty_threshold=0.4, warmup_epochs=10,\n                       difficulty_start_epoch=20,  # NEW PARAMETER\n                       difficulty_weights=None, label_smoothing=0.1):\n    \"\"\"Train with multi-dimensional difficulty assessment and annealing schedule\"\"\"\n    model = model.to(device)\n    \n    # Use label smoothing for better generalization\n    ce_criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n    \n    # Longer warmup for stability\n    warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n        optimizer, start_factor=0.01, total_iters=warmup_epochs\n    )\n    cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=num_epochs - warmup_epochs, eta_min=lr * 0.01\n    )\n    scheduler = torch.optim.lr_scheduler.SequentialLR(\n        optimizer, \n        schedulers=[warmup_scheduler, cosine_scheduler],\n        milestones=[warmup_epochs]\n    )\n    \n    best_val_acc = 0\n    history = {\n        'train_loss': [], 'train_acc': [], \n        'val_loss': [], 'val_acc': [],\n        'difficult_pct': [],\n        'avg_pred_uncertainty': [],\n        'avg_spatial_complexity': [],\n        'avg_temporal_complexity': [],\n        'avg_motion_complexity': [],\n        'difficulty_threshold_used': []  # NEW: track threshold over time\n    }\n    \n    # NEW: Print training schedule\n    print(f\"\\n{'='*60}\")\n    print(f\"Training Schedule:\")\n    print(f\"  Epochs 1-{difficulty_start_epoch}: Pure classification (no difficulty)\")\n    print(f\"  Epochs {difficulty_start_epoch+1}-{num_epochs}: Difficulty-aware training\")\n    print(f\"{'='*60}\\n\")\n    \n    for epoch in range(num_epochs):\n        # NEW: Compute current difficulty threshold (annealing from 0.8 to target)\n        if epoch < difficulty_start_epoch:\n            use_difficulty = False\n            current_threshold = 1.0  # Don't cascade anything\n        else:\n            use_difficulty = True\n            # Anneal from 0.8 to difficulty_threshold over 10 epochs\n            anneal_progress = min(1.0, (epoch - difficulty_start_epoch) / 10)\n            current_threshold = 0.8 - (0.8 - difficulty_threshold) * anneal_progress\n        \n        # Training\n        model.train()\n        train_loss, train_correct, train_total = 0, 0, 0\n        \n        # NEW: Updated progress bar description\n        phase_name = \"Classification\" if not use_difficulty else f\"Difficulty (thresh={current_threshold:.2f})\"\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [{phase_name}]\")\n        \n        for data, labels in pbar:\n            data, labels = data.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            logits = model(data)\n            \n            # Use CrossEntropy with label smoothing instead of Focal Loss during warmup\n            if epoch < warmup_epochs:\n                loss = ce_criterion(logits, labels)\n            else:\n                loss = criterion(logits, labels)\n            \n            loss.backward()\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            if hasattr(model, 'late_dropout'):\n                model.late_dropout.step()\n            \n            train_loss += loss.item()\n            preds = logits.argmax(dim=1)\n            train_correct += (preds == labels).sum().item()\n            train_total += labels.size(0)\n            \n            pbar.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'acc': f'{100*train_correct/train_total:.2f}%'\n            })\n        \n        train_loss /= len(train_loader)\n        train_acc = train_correct / train_total\n        \n        # Validation with difficulty analysis\n        model.eval()\n        val_loss, val_correct, val_total = 0, 0, 0\n        difficult_count = 0\n        \n        # Track difficulty components\n        all_pred_unc, all_spatial, all_temporal, all_motion = [], [], [], []\n        \n        with torch.no_grad():\n            for data, labels in val_loader:\n                data, labels = data.to(device), labels.to(device)\n                \n                # NEW: Always compute difficulty for logging, but only use if active\n                logits, difficulty, components = model(\n                    data, return_difficulty=True, difficulty_weights=difficulty_weights\n                )\n                loss = criterion(logits, labels)\n                \n                val_loss += loss.item()\n                preds = logits.argmax(dim=1)\n                val_correct += (preds == labels).sum().item()\n                val_total += labels.size(0)\n                \n                # Track difficult predictions using current threshold\n                difficult_count += (difficulty > current_threshold).sum().item()\n                \n                # Collect components for analysis\n                all_pred_unc.extend(components['prediction_uncertainty'].cpu().numpy())\n                all_spatial.extend(components['spatial_complexity'].cpu().numpy())\n                all_temporal.extend(components['temporal_complexity'].cpu().numpy())\n                all_motion.extend(components['motion_complexity'].cpu().numpy())\n        \n        val_loss /= len(val_loader)\n        val_acc = val_correct / val_total\n        difficult_pct = 100 * difficult_count / val_total\n        \n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        history['difficult_pct'].append(difficult_pct)\n        history['avg_pred_uncertainty'].append(np.mean(all_pred_unc))\n        history['avg_spatial_complexity'].append(np.mean(all_spatial))\n        history['avg_temporal_complexity'].append(np.mean(all_temporal))\n        history['avg_motion_complexity'].append(np.mean(all_motion))\n        history['difficulty_threshold_used'].append(current_threshold)  # NEW\n        \n        scheduler.step()\n        \n        # NEW: Print more frequently and at key transitions\n        if (epoch + 1) % 5 == 0 or epoch == 0 or epoch == num_epochs - 1 or epoch == difficulty_start_epoch:\n            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {100*train_acc:.2f}%\")\n            print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {100*val_acc:.2f}%\")\n            if use_difficulty:\n                print(f\"  Difficult samples (cascade to next): {difficult_pct:.2f}% (threshold={current_threshold:.2f})\")\n                print(f\"  Difficulty breakdown:\")\n                print(f\"    - Pred uncertainty: {np.mean(all_pred_unc):.3f}\")\n                print(f\"    - Spatial complex:  {np.mean(all_spatial):.3f}\")\n                print(f\"    - Temporal complex: {np.mean(all_temporal):.3f}\")\n                print(f\"    - Motion complex:   {np.mean(all_motion):.3f}\")\n            else:\n                print(f\"  (Difficulty assessment inactive - pure classification phase)\")\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_acc': val_acc,\n                'history': history,\n                'difficulty_weights': difficulty_weights,\n                'difficulty_threshold': difficulty_threshold  # NEW\n            }, 'best_filter_model.pth')\n            if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n                print(f\"  ✓ Saved best model\")\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Training Complete! Best Val Acc: {100*best_val_acc:.2f}%\")\n    print(f\"{'='*60}\\n\")\n    \n    return history\n\n\ndef evaluate_filter_efficiency(model, val_loader, device='cuda', \n                               difficulty_threshold=0.4, difficulty_weights=None):\n    \"\"\"Evaluate with detailed difficulty breakdown\"\"\"\n    model.eval()\n    \n    total = 0\n    easy_count = 0\n    difficult_count = 0\n    \n    easy_correct = 0\n    overall_correct = 0\n    \n    # Track which difficulty component triggers most cascades\n    cascade_reasons = {\n        'high_pred_uncertainty': 0,\n        'high_spatial': 0,\n        'high_temporal': 0,\n        'high_motion': 0\n    }\n    \n    with torch.no_grad():\n        for data, labels in val_loader:\n            data = data.to(device)\n            labels = labels.to(device)\n            \n            logits, difficulty, components = model(\n                data, return_difficulty=True, difficulty_weights=difficulty_weights\n            )\n            preds = logits.argmax(dim=1)\n            \n            batch_size = data.size(0)\n            total += batch_size\n            \n            # Track filtering\n            easy_mask = difficulty <= difficulty_threshold\n            easy_count += easy_mask.sum().item()\n            difficult_count += (~easy_mask).sum().item()\n            \n            # Track accuracy\n            easy_correct += ((preds == labels) & easy_mask).sum().item()\n            overall_correct += (preds == labels).sum().item()\n            \n            # Analyze why samples are difficult\n            for i in range(batch_size):\n                if difficulty[i] > difficulty_threshold:\n                    if components['prediction_uncertainty'][i] > 0.5:\n                        cascade_reasons['high_pred_uncertainty'] += 1\n                    if components['spatial_complexity'][i] > 0.5:\n                        cascade_reasons['high_spatial'] += 1\n                    if components['temporal_complexity'][i] > 0.5:\n                        cascade_reasons['high_temporal'] += 1\n                    if components['motion_complexity'][i] > 0.5:\n                        cascade_reasons['high_motion'] += 1\n    \n    easy_acc = easy_correct / easy_count if easy_count > 0 else 0\n    overall_acc = overall_correct / total\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Multi-Dimensional Difficulty Cascade Analysis\")\n    print(f\"{'='*60}\")\n    print(f\"Total samples: {total}\")\n    print(f\"Easy (handled by filter): {easy_count} ({100*easy_count/total:.1f}%)\")\n    print(f\"  → Accuracy on easy: {100*easy_acc:.2f}%\")\n    print(f\"Difficult (cascade to main): {difficult_count} ({100*difficult_count/total:.1f}%)\")\n    print(f\"\\nCascade reasons (samples can have multiple):\")\n    print(f\"  - High prediction uncertainty: {cascade_reasons['high_pred_uncertainty']}\")\n    print(f\"  - High spatial complexity: {cascade_reasons['high_spatial']}\")\n    print(f\"  - High temporal complexity: {cascade_reasons['high_temporal']}\")\n    print(f\"  - High motion complexity: {cascade_reasons['high_motion']}\")\n    print(f\"\\nOverall filter accuracy: {100*overall_acc:.2f}%\")\n    print(f\"{'='*60}\\n\")\n    \n    return {\n        'total': total,\n        'easy': easy_count,\n        'difficult': difficult_count,\n        'easy_acc': easy_acc,\n        'overall_acc': overall_acc,\n        'cascade_reasons': cascade_reasons\n    }\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nif __name__ == \"__main__\":\n    \n    set_seed(42)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f\"Using device: {device}\")\n    \n    # NEW: Sample dataset to 50% while preserving class distribution\n    sampled_train_csv = stratified_sample_dataset(train_csv, sample_ratio=0.5, random_state=42)\n    \n    # Create train/val split on SAMPLED data\n    split_idx = int(len(sampled_train_csv) * 0.8)\n    train_df = sampled_train_csv[:split_idx]\n    val_df = sampled_train_csv[split_idx:]\n    \n    print(f\"\\nDataset: {len(train_df)} train, {len(val_df)} val\")\n    \n    # Create data loaders\n    train_loader, val_loader, focal_loss, class_weights = create_dataloaders(\n        train_df, val_df, label_map, \n        batch_size=32, max_frames=64, num_workers=4\n    )\n    \n    # Initialize model with PROPER size for 250 classes\n    num_classes = len(label_map)\n    \n    # Find optimal dim that fits under 25MB\n    # For 250 classes, we need bigger model than 96 dim\n    for test_dim in [192, 160, 128, 96]:\n        test_model = ASLFilterModel(\n            num_classes=num_classes,\n            max_len=64,\n            channels=126,\n            dim=test_dim,\n            dropout_step=5000\n        )\n        test_size = test_model.get_model_size_mb()\n        print(f\"Testing dim={test_dim}: {test_size:.2f} MB\")\n        if test_size < 24:  # Leave 1MB margin\n            optimal_dim = test_dim\n            break\n        del test_model\n    \n    print(f\"Selected optimal dim: {optimal_dim}\")\n    \n    # Create final model\n    filter_model = ASLFilterModel(\n        num_classes=num_classes,\n        max_len=64,\n        channels=126,\n        dim=optimal_dim,\n        dropout_step=5000\n    )\n    \n    model_size = filter_model.get_model_size_mb()\n    print(f\"\\n{'='*60}\")\n    print(f\"ASL Filter Model (Multi-Dimensional Difficulty)\")\n    print(f\"Size: {model_size:.2f} MB / 25 MB limit\")\n    print(f\"Parameters: {sum(p.numel() for p in filter_model.parameters()):,}\")\n    print(f\"Dimension: {optimal_dim}\")\n    print(f\"{'='*60}\\n\")\n    assert model_size < 25, f\"Model too large: {model_size:.2f} MB\"\n    \n    # Custom difficulty weights (tune these based on your data)\n    difficulty_weights = {\n        'prediction_uncertainty': 0.35,  # Model confidence\n        'spatial_complexity': 0.25,      # Hand coordination\n        'temporal_complexity': 0.25,     # Motion smoothness\n        'motion_complexity': 0.15        # 3D motion patterns\n    }\n    \n    # Train with adjusted learning rate for larger model\n    initial_lr = 5e-4 if optimal_dim >= 160 else 1e-3\n    \n    print(f\"Starting training with lr={initial_lr}...\\n\")\n    \n    # NEW: Train with REDUCED epochs and difficulty annealing\n    history = train_filter_model(\n        model=filter_model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        criterion=focal_loss,\n        num_epochs=40,  # CHANGED from 200\n        lr=initial_lr,\n        device=device,\n        difficulty_threshold=0.4,\n        warmup_epochs=10,\n        difficulty_start_epoch=20,  # NEW PARAMETER\n        difficulty_weights=difficulty_weights\n    )\n    \n    # Load best and evaluate\n    checkpoint = torch.load('best_filter_model.pth')\n    filter_model.load_state_dict(checkpoint['model_state_dict'])\n    \n    results = evaluate_filter_efficiency(\n        filter_model, val_loader, device=device,\n        difficulty_threshold=0.4,\n        difficulty_weights=difficulty_weights\n    )\n    \n    # Enhanced visualization\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    epochs = range(1, len(history['train_loss']) + 1)\n    \n    # Loss\n    axes[0, 0].plot(epochs, history['train_loss'], label='Train')\n    axes[0, 0].plot(epochs, history['val_loss'], label='Val')\n    axes[0, 0].set_title('Loss')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True)\n    \n    # Accuracy\n    axes[0, 1].plot(epochs, [100*x for x in history['train_acc']], label='Train')\n    axes[0, 1].plot(epochs, [100*x for x in history['val_acc']], label='Val')\n    axes[0, 1].set_title('Accuracy (%)')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True)\n    \n    # Cascade percentage\n    axes[0, 2].plot(epochs, history['difficult_pct'], color='red')\n    axes[0, 2].set_title('% Samples Cascaded to Next Layer')\n    axes[0, 2].set_xlabel('Epoch')\n    axes[0, 2].grid(True)\n    \n    # Difficulty components over time\n    axes[1, 0].plot(epochs, history['avg_pred_uncertainty'], label='Pred Uncertainty')\n    axes[1, 0].plot(epochs, history['avg_spatial_complexity'], label='Spatial')\n    axes[1, 0].plot(epochs, history['avg_temporal_complexity'], label='Temporal')\n    axes[1, 0].plot(epochs, history['avg_motion_complexity'], label='Motion')\n    axes[1, 0].set_title('Difficulty Components')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True)\n    \n    # NEW: Difficulty threshold annealing\n    axes[1, 1].plot(epochs, history['difficulty_threshold_used'], color='purple')\n    axes[1, 1].set_title('Difficulty Threshold (Annealing)')\n    axes[1, 1].set_xlabel('Epoch')\n    axes[1, 1].set_ylabel('Threshold')\n    axes[1, 1].grid(True)\n    \n    # Cascade breakdown\n    cascade_data = results['cascade_reasons']\n    axes[1, 2].bar(cascade_data.keys(), cascade_data.values())\n    axes[1, 2].set_title('Cascade Reasons (Final Model)')\n    axes[1, 2].tick_params(axis='x', rotation=45)\n    axes[1, 2].grid(True, axis='y')\n    \n    plt.tight_layout()\n    plt.savefig('filter_training_analysis.png', dpi=150)\n    print(\"Saved training analysis plot to 'filter_training_analysis.png'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}